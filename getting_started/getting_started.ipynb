{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üöÄ Your First AI Creation\n",
    "\n",
    "This is the `getting_started.ipynb` notebook from the **Generative AI API Starter Pack**.\n",
    "\n",
    "This guide is designed for you to get a result in just a few minutes, even if you have no coding experience. We'll use this notebook to:\n",
    "* **Securely** load your API key.\n",
    "* Write a simple command to get your first AI-generated response.\n",
    "* Celebrate your first successful creation!\n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### 1. ‚öôÔ∏è Install Packages\n",
    "\n",
    "The cell below ensures you have the right tools installed for your API to work. All you need to do is just **click it** and **run it**.  \n",
    "\n",
    "To Run a cell: \n",
    "- Click the cell\n",
    "- Click the \"Run Cell\" button (‚ñ∂Ô∏è) that appears.\n",
    "- You're Done! Now, you're ready for the next step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (1.109.1)\n",
      "Requirement already satisfied: requests in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (2.11.9)\n",
      "Requirement already satisfied: sniffio in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from openai->-r ../requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai->-r ../requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r ../requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r ../requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ../requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r ../requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r ../requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai->-r ../requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from requests->-r ../requirements.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kevingray/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages (from requests->-r ../requirements.txt (line 2)) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# If developing locally, set the path to requirements.txt (relative to notebook/script location)\n",
    "# Use the existing variable 'filename' as defined elsewhere in the notebook\n",
    "filename = \"../requirements.txt\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    pip_command = f\"pip install -r {filename}\"\n",
    "else:\n",
    "    print(f\"No {filename} found, skipping general installation.\")\n",
    "    print(\"Trying to install openai package directly...\")\n",
    "    pip_command = \"pip install openai\"\n",
    "\n",
    "# Execute the appropriate pip command using IPython's run_line_magic\n",
    "!{pip_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2. üîë Securely Add Your API Key\n",
    "\n",
    "Your API key is a password. It is never saved in the notebook itself.\n",
    "\n",
    "- If you are in Colab: Use the üîí \"Secrets\" panel on the left to securely add your key. Name it `OPENAI_API_KEY`.\n",
    "\n",
    "- If you are in Jupyter (locally): Follow the instructions in the `env.examples` to add your API Key to the `.env` file or you will nned to enter it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key set securely for this session!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Checks if the OPENAI_API_KEY environment variable is set\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ Found API key from environment or Colab Secret!\")\n",
    "else:\n",
    "    # If OPENAI_API_KEY is not found, prompts the user to input it securely\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key (input hidden): \")\n",
    "    print(\"‚úÖ API key set securely for this session!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 3. ‚úçÔ∏è Write Your First Prompt\n",
    "\n",
    "Now for the fun part! Write your request for the AI below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the text inside the quotes to make your own request.\n",
    "my_prompt = (\n",
    "    \"Generate a short, funny story about a robot who discovers a love for gardening.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 4. üß† Get a Response from the AI\n",
    "\n",
    "This cell will send your prompt to the AI and get a response. Just run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'fault': {'faultstring': 'Failed to resolve API Key variable request.header.api-key', 'detail': {'errorcode': 'steps.oauth.v2.FailedToResolveAPIKey'}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m client = OpenAI(\n\u001b[32m      6\u001b[39m     api_key=os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     base_url=os.environ.get(\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     ),\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Create a chat completion with a system prompt using the \"gpt-4\" model.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# There are other models available, such as \"gpt-3.5-turbo\" or \"gpt-4\" which\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# may have different strengths (i.e. speed, cost, capabilities) for your use case.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# See the 'Resources' section below for more information on models and APIs.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Success! You have received a response from the AI.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebase/harvard-atg/generative-ai-api-starter-notebook/.venv/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'fault': {'faultstring': 'Failed to resolve API Key variable request.header.api-key', 'detail': {'errorcode': 'steps.oauth.v2.FailedToResolveAPIKey'}}}"
     ]
    }
   ],
   "source": [
    "# The code below is the API interface that talks to the AI service. Don't worry about the details!\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with the Harvard API Portal base url and your API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ.get(\n",
    "        \"OPENAI_BASE_URL\",\n",
    "        \"https://go.apis.huit.harvard.edu/ais-openai-direct-limited-schools/v1\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a chat completion with a system prompt using the \"gpt-4\" model.\n",
    "# There are other models available, such as \"gpt-3.5-turbo\" or \"gpt-4\" which\n",
    "# may have different strengths (i.e. speed, cost, capabilities) for your use case.\n",
    "# See the 'Resources' section below for more information on models and APIs.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": my_prompt}]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Success! You have received a response from the AI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 5. üéâ See the Result!\n",
    "\n",
    "Run the next to see what the AI generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the AI-generated story.\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 6. Add More Power: Prompts and Paramaters \n",
    "\n",
    "Congratulations, you've successfully completed your first AI creation!\n",
    "\n",
    "Now that you've mastered the basics of using an API, lets try two more activites that make using APIs really powerful:\n",
    "- Using System Prompts\n",
    "- Changing API Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### A. üìù Add a \"System Prompt\" (Make Your ChatBot)\n",
    "\n",
    "Congratulations, you've successfully completed your first AI creation!  Let's just right into something else fun. Let's make a ChatBot using system prompts.\n",
    "\n",
    "System prompts are like master instructions that tell the AI **how to behave** before you ask a question! This is a simple but powerful technique to guide the AI for better results. \n",
    "\n",
    "This is the foundation of **prompt engineering**. See the [Resources]() for more resources on prompt engineering. \n",
    "\n",
    "><br>    \n",
    "> <h4>Example: Choice the 'Profession' of your ChatBot</h4>\n",
    "> \n",
    "> A great example of a system prompt where changing even a a single keyword gives vastly different results \n",
    "> \n",
    "> \"You are a helpful assistant who always answers as a [profession].\"\n",
    "> \n",
    "> For example, if you change [profession] to \"doctor\", \"pirate\", \"stand-up comedian\", or \"Shakespearean > actor\", the AI's answers to the same user question will be completely different in tone, style, and content.\n",
    "> \n",
    "> Try changing the profession in the `system_prompt` below.\n",
    ">\n",
    ">  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# The system prompt sets the tone and rules for the AI's response.\n",
    "# You can change the text inside the quotes to give the AI any role you want!\n",
    "system_prompt = \"You are a helpful assistant who always answers as a doctor.\"\n",
    "\n",
    "# This is the user's question, which the AI will respond to.\n",
    "user_question = \"Explain how a volcano erupts.\"\n",
    "\n",
    "# This is the main API call that sends your prompts to the AI model.\n",
    "# We shouldn't need to change anything here because it's already set up to use the system prompt.\n",
    "response = client.chat.completions.create(\n",
    "    # 'model' specifies which AI model you want to use.\n",
    "    # For example, you can choose from various models like \"gpt-3.5-turbo\" or \"gpt-4\".\n",
    "    # Different models have different capabilities (i.e. speed, cost, performance), and some.\n",
    "    # may be better suited for certain tasks than others.\n",
    "    # To learn more about the available models, see the 'OpenAI Models' link in the 'Resources' section below.\n",
    "    model=\"gpt-4\",\n",
    "    # 'messages' is the list of prompts you are sending to the AI.\n",
    "    # The system prompt comes first, followed by the user's question.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_question},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# This final line prints the AI's response to your screen.\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### B. üé® Adjust Parameters\n",
    "\n",
    "You can fine-tune the AI's responses by changing the parameter settings. For example, these parameters allow you to control **creativity** and **response length**.\n",
    "\n",
    "* **`temperature`**: This controls the creativity or randomness of the output.\n",
    "    * Lower values (e.g., 0.2) make the response more predictable and focused.\n",
    "    * Higher values (e.g., 0.8) make the response more surprising and varied.\n",
    "    * For OpenAI models, a good starting point is 0.5.\n",
    "    * To learnmore about selecting the best tempatures for your use case, see:\n",
    "      * The **Tempature Cheat Sheet** in the [Resources](#resources) section below.\n",
    "  \n",
    "* **`max_completion_tokens`**: This sets a hard limit on the length of the response, which is useful for controlling cost and response size.\n",
    "    * A higher value allows for a longe response, while a lower value keeps it brief.\n",
    "    * The maximum value depends on the model you are using. For example, \"gpt-3.5-turbo\" has a max of 4096 tokens, while \"gpt-4\" can handle up to 8192 tokens.\n",
    "    * A good starting point for many applications is 150 tokens. You can adjust this based on your needs.\n",
    "\n",
    "You can learn more in the official [OpenAI Chat Completions Parameters documentation](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- You can adjust these settings to fine-tune the AI's response ---\n",
    "\n",
    "# 'temperature' controls creativity. \n",
    "# A higher value makes the AI's response more random.\n",
    "# A lower value makes it more focused and deterministic.\n",
    "# For OpenAI models, a good starting point is 0.5.\n",
    "temperature = 0.5\n",
    "\n",
    "# 'max_tokens' sets a limit on the length of the response.\n",
    "# A higher value allows for a longer response, while a lower value keeps it brief.\n",
    "# The maximum value depends on the model you are using.\n",
    "# For this example, let's start at 150 tokens. You can adjust this based on your needs.\n",
    "max_tokens = 150\n",
    "\n",
    "# The system prompt sets the tone and rules for the AI's response.\n",
    "# You can change the text inside the quotes to give the AI any role you want!\n",
    "system_prompt = \"You are a helpful assistant who always answers as a doctor.\"\n",
    "\n",
    "# This is the user's question, which the AI will respond to.\n",
    "# Feel free to try different questions!\n",
    "user_question = \"Explain how a volcano erupts.\"\n",
    "\n",
    "# This is the main API call, now with the added parameters.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_question},\n",
    "    ],\n",
    "    # 'temperature' and 'max_tokens' are the optional parameters you can adjust.\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    ")\n",
    "\n",
    "# This prints the AI's response to your screen.\n",
    "print(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 7. What's Next?\n",
    "\n",
    "You now have experience with some powerful features of the OpenAI API! With the tips and resources below, you are on a great path to building apps, tools, and experiments with the power of Generative AI! Happy building!\n",
    "\n",
    "##### **Tip: Jupyter & Colab Basics**\n",
    "Here are some quick tips to help your navigate your notebook even more quickly.\n",
    "- ‚ñ∂Ô∏è **Run a cell:** Click the play button at the left of the cell, or press `Shift + Enter`\n",
    "- ‚ûï **Add a new cell:** Click \"+ Code\" or \"+ Text\" at the top, or press `Ctrl + M` then `B`\n",
    "- ‚úèÔ∏è **Edit a cell:** Just click inside and start typing\n",
    "- üîÑ **Rerun a cell:** Click it again and press `Shift + Enter`\n",
    "- üìù **Change cell type:** Use the dropdown at the top (Code or Markdown/Text)\n",
    "- üóëÔ∏è **Delete a cell:** Select the cell, then click the trash can icon or press `Ctrl + M` then `D`\n",
    "- üíæ **Save notebook:** File ‚Üí Save, or press `Ctrl + S`\n",
    "- üîÅ **Restart the notebook:** Runtime ‚Üí Restart runtime/Restart & Run all (useful if things get stuck!)\n",
    "\n",
    "##### **Resources**\n",
    "Here are some useful resources that developers (like you) appreciate as they build and applications.  \n",
    "- Learn more about OpenAI API:\n",
    "  - [OpenAI Official Documenation](https://platform.openai.com/docs/overview)\n",
    "    - [OpenAI Models](https://platform.openai.com/docs/models)\n",
    "  - [OpenAI Cookbooks](https://cookbook.openai.com/about)\n",
    "  - [Temperature Cheat Sheet (OpenAI Developer Community)](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683)\n",
    "- Learn more about Prompt Engineering:\n",
    "  - [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "  - [Harvard: Getting started with prompts for text-based Generative AI tools](https://www.huit.harvard.edu/news/ai-prompts)\n",
    "  - [Harvard: System Prompt Library](https://github.com/ncwilson78/System-Prompt-Library)\n",
    "  - [Harvard: Teaching Effectively with ChatGPT Prompts](https://docs.google.com/document/d/1EJTKJ1sKcNAV_TnPcklKHB9-BaCeojNMUJeOJGLgPyY/edit?tab=t.0#heading=h.cv1kxhgzmhkc)\n",
    "-  GitHub: Learn how to share your code\n",
    "  - [Github: Start Your Journey](https://docs.github.com/en/get-started/start-your-journey)\n",
    "- Develop in Colab:\n",
    "  - [Welcome to Colab](https://colab.research.google.com/?hl=en-GB)\n",
    "  - [Python Tutorial With Google Colab](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb)\n",
    "- Develop Locally: \n",
    "  - [Getting Started with VS Code](https://code.visualstudio.com/docs/introvideos/basics)\n",
    "  - [Getting Started with Python in VS Code](https://www.youtube.com/watch?v=D2cwvpJSBX4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
